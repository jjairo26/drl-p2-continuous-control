{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5312ca",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPGAgent\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "BUFFER_SIZE = 100000    # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 3e-5         # learning rate of the actor\n",
    "LR_CRITIC = 4e-4        # learning rate of the critic\n",
    "HIDDEN_SIZE_1 = 256     # size of the first hidden layer\n",
    "HIDDEN_SIZE_2 = 256     # size of the second hidden layer\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "LEARN_EVERY = 40        # learn every x steps\n",
    "NUM_TRAINING = 20       # number of training iterations per update\n",
    "\n",
    "# Other parameters\n",
    "n_episodes = 800\n",
    "t_max = 900  # maximum number of timesteps per episode\n",
    "scores = []  # list of scores from each episode\n",
    "mean_scores = []  # mean score at each episode\n",
    "scores_window = deque(maxlen=100)\n",
    "high_score_count = 0  # count of episodes with high score of 30 or more\n",
    "\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, hidden_size_1=HIDDEN_SIZE_1,\n",
    "                 hidden_size_2=HIDDEN_SIZE_2, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA,\n",
    "                 tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY, learn_every=LEARN_EVERY,\n",
    "                 num_training=NUM_TRAINING, random_seed=42)\n",
    "\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    # Intialize a random process for action exploration\n",
    "    agent.noise_reset()  # reset the noise process for each episode\n",
    "    # Receive initial state and reset scores\n",
    "    env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "    state = env_info.vector_observations[0]  # get the current state (for each agent)\n",
    "    score = 0  # initialize the score \n",
    "\n",
    "    for t in range(1, t_max + 1):\n",
    "        # Select an action according to the current policy and exploration noise\n",
    "        action = agent.act(state, add_noise=True)\n",
    "\n",
    "        # Execute the action and observe the next state and reward\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]  # get next state \n",
    "        reward = env_info.rewards[0]  # get reward \n",
    "        done = env_info.local_done[0]  # get done \n",
    "\n",
    "\n",
    "        # Add to experience and learn\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(score)\n",
    "    scores_window.append(score)  \n",
    "    mean_scores.append(np.mean(scores_window))  \n",
    "    print(f\"\\rEpisode {i_episode}/{n_episodes} - Mean Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        print(f\"\\rEpisode {i_episode} - Mean Score: {np.mean(scores_window)}\")\n",
    "\n",
    "    if np.mean(scores_window) >= 30.0:\n",
    "        high_score_count += 1\n",
    "        if high_score_count >= 100:\n",
    "            print(f\"Environment solved in {i_episode} episodes!\")\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2c92",
   "metadata": {},
   "source": [
    "### Plot scores and save along with hyperparameters in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(211)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "fig.add_subplot(212)\n",
    "plt.plot(np.arange(len(mean_scores)), mean_scores)\n",
    "plt.ylabel('Mean Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44222bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the scores and parameters to a JSON file\n",
    "data = {\n",
    "    'BUFFER_SIZE': BUFFER_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR_ACTOR': LR_ACTOR,\n",
    "    'LR_CRITIC': LR_CRITIC,\n",
    "    'HIDDEN_SIZE_1': HIDDEN_SIZE_1,\n",
    "    'HIDDEN_SIZE_2': HIDDEN_SIZE_2,\n",
    "    'WEIGHT_DECAY': WEIGHT_DECAY,\n",
    "    'LEARN_EVERY': LEARN_EVERY,\n",
    "    'NUM_TRAINING': NUM_TRAINING,\n",
    "    'OU_mean': agent.noise.mu.tolist(),\n",
    "    'OU_theta': agent.noise.theta,\n",
    "    'OU_sigma': agent.noise.sigma,\n",
    "    'scores': scores,\n",
    "    'mean_scores': mean_scores\n",
    "}\n",
    "\n",
    "with open('training_scores.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf0b10",
   "metadata": {},
   "source": [
    "## Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf35ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "from ddpg_agent import DDPGAgent\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Load parameters and scores from the JSON file\n",
    "with open('training_scores.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Example: Access hyperparameters\n",
    "BUFFER_SIZE = data['BUFFER_SIZE']\n",
    "BATCH_SIZE = data['BATCH_SIZE']\n",
    "GAMMA = data['GAMMA']\n",
    "TAU = data['TAU']\n",
    "LR_ACTOR = data['LR_ACTOR']\n",
    "LR_CRITIC = data['LR_CRITIC']\n",
    "HIDDEN_SIZE_1 = data['HIDDEN_SIZE_1']\n",
    "HIDDEN_SIZE_2 = data['HIDDEN_SIZE_2']\n",
    "WEIGHT_DECAY = data['WEIGHT_DECAY']\n",
    "LEARN_EVERY = data['LEARN_EVERY']\n",
    "NUM_TRAINING = data['NUM_TRAINING']\n",
    "\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, hidden_size_1=HIDDEN_SIZE_1,\n",
    "                  hidden_size_2=HIDDEN_SIZE_2, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA,\n",
    "                  tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY,\n",
    "                  learn_every=LEARN_EVERY, num_training=NUM_TRAINING, random_seed=42)\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location=torch.device('cpu')))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "\n",
    "t_max = 900\n",
    "\n",
    "for t in range(1, t_max + 1):\n",
    "    action = agent.act(state, add_noise=False)  # No noise for evaluation\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Score achieved by the trained agent: {score}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd-p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
